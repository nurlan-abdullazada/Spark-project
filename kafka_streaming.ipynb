{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0027efa6-c10b-4771-b0ec-76754386b168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved schema: {'type': 'record', 'name': 'User', 'fields': [{'name': 'id', 'type': 'int'}, {'name': 'name', 'type': 'string'}, {'name': 'email', 'type': 'string'}, {'name': 'age', 'type': 'int'}]}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.avro.functions import from_avro\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Function to get schema from Schema Registry\n",
    "def get_avro_schema_from_registry(schema_registry_url, subject_name):\n",
    "    url = f\"{schema_registry_url}/subjects/{subject_name}/versions/latest\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return json.loads(response.json()['schema'])\n",
    "\n",
    "# Configuration\n",
    "SCHEMA_REGISTRY_URL = \"http://schema-registry:8081\"\n",
    "SUBJECT_NAME = \"user-topic-value\"\n",
    "\n",
    "# Get schema from registry\n",
    "schema_json = get_avro_schema_from_registry(SCHEMA_REGISTRY_URL, SUBJECT_NAME)\n",
    "print(f\"Retrieved schema: {schema_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e303088f-9f50-428d-8c93-f4a3973507a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-87dee02c-2e65-460b-a411-905beb2aced9;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.5.0 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      ":: resolution report :: resolve 1598ms :: artifacts dl 74ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-87dee02c-2e65-460b-a411-905beb2aced9\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/5ms)\n",
      "25/09/02 16:18:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/02 16:18:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully!\n",
      "Spark UI available at: http://localhost:4040\n"
     ]
    }
   ],
   "source": [
    "# Create Spark session with Avro support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaAvroToPostgreSQL\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \",\".join([\n",
    "        \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\",\n",
    "        \"org.apache.spark:spark-avro_2.12:3.5.0\"\n",
    "    ])) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session created successfully!\")\n",
    "print(f\"Spark UI available at: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "873b1d59-c63a-41b2-87e7-5fba02966f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kafka stream configured with Avro deserialization\n"
     ]
    }
   ],
   "source": [
    "# Read from Kafka\n",
    "df_kafka = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"user-topic\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Remove Confluent wire format header (first 5 bytes) and deserialize Avro\n",
    "df_avro = df_kafka.withColumn(\n",
    "    \"avro_payload\", \n",
    "    expr(\"substring(value, 6, length(value)-5)\")\n",
    ").select(\n",
    "    col(\"key\").cast(\"string\").alias(\"message_key\"),\n",
    "    from_avro(\"avro_payload\", json.dumps(schema_json)).alias(\"user_data\"),\n",
    "    col(\"topic\").alias(\"topic\"),\n",
    "    col(\"partition\").alias(\"partition_id\"),\n",
    "    col(\"offset\").alias(\"offset_value\"),\n",
    "    col(\"timestamp\").alias(\"timestamp_value\")\n",
    ").select(\n",
    "    \"message_key\",\n",
    "    \"user_data.*\",  # Expand the user_data struct\n",
    "    \"topic\",\n",
    "    \"partition_id\", \n",
    "    \"offset_value\",\n",
    "    \"timestamp_value\"\n",
    ")\n",
    "\n",
    "print(\"Kafka stream configured with Avro deserialization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "226a455a-a3ad-4d68-9985-97139f757068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/02 16:20:29 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming started! Send Avro messages to see them processed...\n",
      "To stop: run query.stop() in the next cell\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/02 16:20:32 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    }
   ],
   "source": [
    "# Function to write each batch to PostgreSQL\n",
    "def write_to_postgres(batch_df, batch_id):\n",
    "    if batch_df.count() > 0:\n",
    "        batch_df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", \"jdbc:postgresql://postgres:5432/sparkdb\") \\\n",
    "            .option(\"dbtable\", \"kafka_messages\") \\\n",
    "            .option(\"user\", \"sparkuser\") \\\n",
    "            .option(\"password\", \"sparkpass\") \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "        print(f\"Batch {batch_id}: Wrote {batch_df.count()} Avro records to PostgreSQL\")\n",
    "\n",
    "# Start the streaming query\n",
    "query = df_avro.writeStream \\\n",
    "    .foreachBatch(write_to_postgres) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kafka-avro-postgres-checkpoint\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Streaming started! Send Avro messages to see them processed...\")\n",
    "print(\"To stop: run query.stop() in the next cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fcfce5-f5ef-43a7-84da-4cf96f814de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To stop the stream when done\n",
    "# query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
